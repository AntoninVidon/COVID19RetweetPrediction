{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simon-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor # gradient boosting regression\n",
    "from verstack.stratified_continuous_split import scsplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import os # to set the right working directory\n",
    "from tqdm import tqdm # to use convenient progress bars\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"../Preprocessing/Data/train_processed.pkl\")\n",
    "eval_data = pd.read_pickle(\"../Preprocessing/Data/eval_processed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling user_followers_count\n",
    "train_data['user_followers_count'] = preprocessing.scale(train_data['user_followers_count'])\n",
    "eval_data['user_followers_count'] = preprocessing.scale(eval_data['user_followers_count'])\n",
    "    \n",
    "# Scaling user_friends_count  \n",
    "train_data['user_friends_count'] = preprocessing.scale(train_data['user_friends_count'])\n",
    "eval_data['user_friends_count'] = preprocessing.scale(eval_data['user_friends_count'])\n",
    "  \n",
    "# Scaling user_statuses_count     \n",
    "train_data['user_statuses_count'] = preprocessing.scale(train_data['user_statuses_count'])\n",
    "eval_data['user_statuses_count'] = preprocessing.scale(eval_data['user_statuses_count'])\n",
    "    \n",
    "# Scaling text_length\n",
    "train_data['text_length'] = preprocessing.scale(train_data['text_length'])\n",
    "eval_data['text_length'] = preprocessing.scale(eval_data['text_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], \n",
    "                                           train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['user_statuses_count', 'hashtag_count', 'user_mentions_count', 'user_followers_count', 'user_friends_count', 'user_verified', 'text_length', 'hour', 'week_day', 'day']]\n",
    "X_test = X_test[['user_statuses_count', 'hashtag_count', 'user_mentions_count', 'user_followers_count', 'user_friends_count', 'user_verified', 'text_length', 'hour', 'week_day', 'day']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training model on X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "373/373 - 1s - loss: 144.5935 - mae: 144.5935 - mse: 7907016.5000 - val_loss: 149.8457 - val_mae: 149.8457 - val_mse: 8758844.0000\n",
      "Epoch 2/100\n",
      "373/373 - 0s - loss: 143.7017 - mae: 143.7017 - mse: 7881909.0000 - val_loss: 149.2675 - val_mae: 149.2675 - val_mse: 8743669.0000\n",
      "Epoch 3/100\n",
      "373/373 - 0s - loss: 143.0706 - mae: 143.0706 - mse: 7861635.0000 - val_loss: 148.6145 - val_mae: 148.6145 - val_mse: 8714088.0000\n",
      "Epoch 4/100\n",
      "373/373 - 0s - loss: 142.4017 - mae: 142.4017 - mse: 7840991.5000 - val_loss: 148.1813 - val_mae: 148.1813 - val_mse: 8693237.0000\n",
      "Epoch 5/100\n",
      "373/373 - 0s - loss: 141.7308 - mae: 141.7308 - mse: 7816236.5000 - val_loss: 147.5374 - val_mae: 147.5374 - val_mse: 8646232.0000\n",
      "Epoch 6/100\n",
      "373/373 - 0s - loss: 141.1279 - mae: 141.1279 - mse: 7782096.5000 - val_loss: 146.7651 - val_mae: 146.7651 - val_mse: 8617819.0000\n",
      "Epoch 7/100\n",
      "373/373 - 0s - loss: 140.6881 - mae: 140.6881 - mse: 7757587.0000 - val_loss: 146.3625 - val_mae: 146.3625 - val_mse: 8590714.0000\n",
      "Epoch 8/100\n",
      "373/373 - 0s - loss: 140.4148 - mae: 140.4148 - mse: 7741928.0000 - val_loss: 146.3228 - val_mae: 146.3228 - val_mse: 8568497.0000\n",
      "Epoch 9/100\n",
      "373/373 - 0s - loss: 140.2687 - mae: 140.2687 - mse: 7727884.0000 - val_loss: 146.0715 - val_mae: 146.0715 - val_mse: 8569083.0000\n",
      "Epoch 10/100\n",
      "373/373 - 0s - loss: 140.0843 - mae: 140.0843 - mse: 7722337.0000 - val_loss: 145.7643 - val_mae: 145.7643 - val_mse: 8550030.0000\n",
      "Epoch 11/100\n",
      "373/373 - 0s - loss: 140.0151 - mae: 140.0151 - mse: 7716194.0000 - val_loss: 145.6091 - val_mae: 145.6091 - val_mse: 8528763.0000\n",
      "Epoch 12/100\n",
      "373/373 - 0s - loss: 139.9239 - mae: 139.9239 - mse: 7713457.0000 - val_loss: 145.4897 - val_mae: 145.4897 - val_mse: 8529277.0000\n",
      "Epoch 13/100\n",
      "373/373 - 1s - loss: 139.8908 - mae: 139.8908 - mse: 7709734.5000 - val_loss: 145.8915 - val_mae: 145.8915 - val_mse: 8521350.0000\n",
      "Epoch 14/100\n",
      "373/373 - 1s - loss: 139.8156 - mae: 139.8156 - mse: 7708752.5000 - val_loss: 145.5228 - val_mae: 145.5228 - val_mse: 8543806.0000\n",
      "Epoch 15/100\n",
      "373/373 - 0s - loss: 139.7525 - mae: 139.7525 - mse: 7702115.0000 - val_loss: 145.5436 - val_mae: 145.5436 - val_mse: 8531375.0000\n",
      "Epoch 16/100\n",
      "373/373 - 0s - loss: 139.7477 - mae: 139.7477 - mse: 7700734.5000 - val_loss: 145.3476 - val_mae: 145.3476 - val_mse: 8524299.0000\n",
      "Epoch 17/100\n",
      "373/373 - 0s - loss: 139.7069 - mae: 139.7069 - mse: 7703548.5000 - val_loss: 145.4571 - val_mae: 145.4571 - val_mse: 8514047.0000\n",
      "Epoch 18/100\n",
      "373/373 - 0s - loss: 139.6588 - mae: 139.6588 - mse: 7701441.5000 - val_loss: 145.5017 - val_mae: 145.5017 - val_mse: 8549487.0000\n",
      "Epoch 19/100\n",
      "373/373 - 1s - loss: 139.6253 - mae: 139.6253 - mse: 7711703.0000 - val_loss: 145.2637 - val_mae: 145.2637 - val_mse: 8512686.0000\n",
      "Epoch 20/100\n",
      "373/373 - 0s - loss: 139.6269 - mae: 139.6269 - mse: 7703527.0000 - val_loss: 145.1897 - val_mae: 145.1897 - val_mse: 8518490.0000\n",
      "Epoch 21/100\n",
      "373/373 - 0s - loss: 139.5854 - mae: 139.5854 - mse: 7706093.0000 - val_loss: 145.1868 - val_mae: 145.1868 - val_mse: 8506882.0000\n",
      "Epoch 22/100\n",
      "373/373 - 0s - loss: 139.5685 - mae: 139.5685 - mse: 7701213.0000 - val_loss: 145.2064 - val_mae: 145.2064 - val_mse: 8521030.0000\n",
      "Epoch 23/100\n",
      "373/373 - 0s - loss: 139.5323 - mae: 139.5323 - mse: 7699583.0000 - val_loss: 145.1259 - val_mae: 145.1259 - val_mse: 8497736.0000\n",
      "Epoch 24/100\n",
      "373/373 - 0s - loss: 139.4968 - mae: 139.4968 - mse: 7692487.0000 - val_loss: 145.3209 - val_mae: 145.3209 - val_mse: 8542875.0000\n",
      "Epoch 25/100\n",
      "373/373 - 0s - loss: 139.5204 - mae: 139.5204 - mse: 7697622.0000 - val_loss: 145.2277 - val_mae: 145.2277 - val_mse: 8528880.0000\n",
      "Epoch 26/100\n",
      "373/373 - 0s - loss: 139.4690 - mae: 139.4690 - mse: 7699445.0000 - val_loss: 145.3209 - val_mae: 145.3209 - val_mse: 8541242.0000\n",
      "Epoch 27/100\n",
      "373/373 - 0s - loss: 139.3415 - mae: 139.3415 - mse: 7700173.0000 - val_loss: 145.1089 - val_mae: 145.1089 - val_mse: 8501264.0000\n",
      "Epoch 28/100\n",
      "373/373 - 0s - loss: 139.4163 - mae: 139.4163 - mse: 7699169.5000 - val_loss: 145.1677 - val_mae: 145.1677 - val_mse: 8511333.0000\n",
      "Epoch 29/100\n",
      "373/373 - 0s - loss: 139.4095 - mae: 139.4095 - mse: 7693035.5000 - val_loss: 145.1486 - val_mae: 145.1486 - val_mse: 8522475.0000\n",
      "Epoch 30/100\n",
      "373/373 - 0s - loss: 139.3629 - mae: 139.3629 - mse: 7689405.5000 - val_loss: 145.1663 - val_mae: 145.1663 - val_mse: 8517266.0000\n",
      "Epoch 31/100\n",
      "373/373 - 0s - loss: 139.3267 - mae: 139.3267 - mse: 7688562.5000 - val_loss: 145.3937 - val_mae: 145.3937 - val_mse: 8549395.0000\n",
      "Epoch 32/100\n",
      "373/373 - 0s - loss: 139.4158 - mae: 139.4158 - mse: 7698983.5000 - val_loss: 145.1824 - val_mae: 145.1824 - val_mse: 8532020.0000\n",
      "Epoch 33/100\n",
      "373/373 - 0s - loss: 139.2575 - mae: 139.2575 - mse: 7692555.5000 - val_loss: 145.1373 - val_mae: 145.1373 - val_mse: 8529284.0000\n",
      "Epoch 34/100\n",
      "373/373 - 0s - loss: 139.3049 - mae: 139.3049 - mse: 7695479.5000 - val_loss: 145.0926 - val_mae: 145.0926 - val_mse: 8529904.0000\n",
      "Epoch 35/100\n",
      "373/373 - 0s - loss: 139.3541 - mae: 139.3541 - mse: 7695466.5000 - val_loss: 145.1520 - val_mae: 145.1520 - val_mse: 8519081.0000\n",
      "Epoch 36/100\n",
      "373/373 - 0s - loss: 139.3212 - mae: 139.3212 - mse: 7699053.5000 - val_loss: 145.1915 - val_mae: 145.1915 - val_mse: 8531764.0000\n",
      "Epoch 37/100\n",
      "373/373 - 1s - loss: 139.3226 - mae: 139.3226 - mse: 7699947.0000 - val_loss: 145.1684 - val_mae: 145.1684 - val_mse: 8518419.0000\n",
      "Epoch 38/100\n",
      "373/373 - 1s - loss: 139.3095 - mae: 139.3095 - mse: 7700272.0000 - val_loss: 145.0737 - val_mae: 145.0737 - val_mse: 8515233.0000\n",
      "Epoch 39/100\n",
      "373/373 - 1s - loss: 139.2346 - mae: 139.2346 - mse: 7697085.0000 - val_loss: 144.9828 - val_mae: 144.9828 - val_mse: 8498484.0000\n",
      "Epoch 40/100\n",
      "373/373 - 1s - loss: 139.2160 - mae: 139.2160 - mse: 7694899.0000 - val_loss: 145.0430 - val_mae: 145.0430 - val_mse: 8526909.0000\n",
      "Epoch 41/100\n",
      "373/373 - 1s - loss: 139.2421 - mae: 139.2421 - mse: 7692584.0000 - val_loss: 144.9771 - val_mae: 144.9771 - val_mse: 8507203.0000\n",
      "Epoch 42/100\n",
      "373/373 - 1s - loss: 139.2190 - mae: 139.2190 - mse: 7694982.5000 - val_loss: 144.9922 - val_mae: 144.9922 - val_mse: 8494962.0000\n",
      "Epoch 43/100\n",
      "373/373 - 1s - loss: 139.2306 - mae: 139.2306 - mse: 7690001.5000 - val_loss: 145.0825 - val_mae: 145.0825 - val_mse: 8506960.0000\n",
      "Epoch 44/100\n",
      "373/373 - 1s - loss: 139.1955 - mae: 139.1955 - mse: 7692166.0000 - val_loss: 145.0407 - val_mae: 145.0407 - val_mse: 8500235.0000\n",
      "Epoch 45/100\n",
      "373/373 - 1s - loss: 139.1806 - mae: 139.1806 - mse: 7698977.5000 - val_loss: 145.1657 - val_mae: 145.1657 - val_mse: 8531302.0000\n",
      "Epoch 46/100\n",
      "373/373 - 1s - loss: 139.1781 - mae: 139.1781 - mse: 7696906.5000 - val_loss: 145.0643 - val_mae: 145.0643 - val_mse: 8521151.0000\n",
      "Epoch 47/100\n",
      "373/373 - 1s - loss: 139.1587 - mae: 139.1587 - mse: 7688111.0000 - val_loss: 144.9928 - val_mae: 144.9928 - val_mse: 8530866.0000\n",
      "Epoch 48/100\n",
      "373/373 - 1s - loss: 139.1088 - mae: 139.1088 - mse: 7694079.5000 - val_loss: 144.9749 - val_mae: 144.9749 - val_mse: 8498895.0000\n",
      "Epoch 49/100\n",
      "373/373 - 1s - loss: 139.1338 - mae: 139.1338 - mse: 7689504.5000 - val_loss: 144.9944 - val_mae: 144.9944 - val_mse: 8534980.0000\n",
      "Epoch 50/100\n",
      "373/373 - 1s - loss: 139.1591 - mae: 139.1591 - mse: 7694210.0000 - val_loss: 144.9851 - val_mae: 144.9851 - val_mse: 8535750.0000\n",
      "Epoch 51/100\n",
      "373/373 - 1s - loss: 139.1246 - mae: 139.1246 - mse: 7693624.0000 - val_loss: 144.9438 - val_mae: 144.9438 - val_mse: 8519498.0000\n",
      "Epoch 52/100\n",
      "373/373 - 1s - loss: 139.1303 - mae: 139.1303 - mse: 7696134.0000 - val_loss: 144.9762 - val_mae: 144.9762 - val_mse: 8514996.0000\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/373 - 1s - loss: 139.1365 - mae: 139.1365 - mse: 7692858.5000 - val_loss: 145.0128 - val_mae: 145.0128 - val_mse: 8516425.0000\n",
      "Epoch 54/100\n",
      "373/373 - 1s - loss: 139.0830 - mae: 139.0830 - mse: 7693213.5000 - val_loss: 145.0228 - val_mae: 145.0228 - val_mse: 8534826.0000\n",
      "Epoch 55/100\n",
      "373/373 - 1s - loss: 139.0788 - mae: 139.0788 - mse: 7691302.0000 - val_loss: 145.0635 - val_mae: 145.0635 - val_mse: 8506381.0000\n",
      "Epoch 56/100\n",
      "373/373 - 1s - loss: 139.0619 - mae: 139.0619 - mse: 7688167.5000 - val_loss: 144.9254 - val_mae: 144.9254 - val_mse: 8509805.0000\n",
      "Epoch 57/100\n",
      "373/373 - 1s - loss: 139.0565 - mae: 139.0565 - mse: 7687860.0000 - val_loss: 144.8907 - val_mae: 144.8907 - val_mse: 8509905.0000\n",
      "Epoch 58/100\n",
      "373/373 - 1s - loss: 139.0062 - mae: 139.0062 - mse: 7691004.5000 - val_loss: 144.9101 - val_mae: 144.9101 - val_mse: 8498605.0000\n",
      "Epoch 59/100\n",
      "373/373 - 1s - loss: 138.9944 - mae: 138.9944 - mse: 7689881.5000 - val_loss: 144.8981 - val_mae: 144.8981 - val_mse: 8489138.0000\n",
      "Epoch 60/100\n",
      "373/373 - 1s - loss: 139.0228 - mae: 139.0228 - mse: 7685666.5000 - val_loss: 144.9276 - val_mae: 144.9276 - val_mse: 8532635.0000\n",
      "Epoch 61/100\n",
      "373/373 - 1s - loss: 139.0229 - mae: 139.0229 - mse: 7692209.5000 - val_loss: 144.8489 - val_mae: 144.8489 - val_mse: 8513038.0000\n",
      "Epoch 62/100\n",
      "373/373 - 1s - loss: 138.9799 - mae: 138.9799 - mse: 7683386.0000 - val_loss: 144.8981 - val_mae: 144.8981 - val_mse: 8513287.0000\n",
      "Epoch 63/100\n",
      "373/373 - 1s - loss: 138.9834 - mae: 138.9834 - mse: 7686540.5000 - val_loss: 144.9334 - val_mae: 144.9334 - val_mse: 8538056.0000\n",
      "Epoch 64/100\n",
      "373/373 - 1s - loss: 138.9931 - mae: 138.9931 - mse: 7687599.0000 - val_loss: 144.8676 - val_mae: 144.8676 - val_mse: 8520375.0000\n",
      "Epoch 65/100\n",
      "373/373 - 1s - loss: 138.9118 - mae: 138.9118 - mse: 7687079.5000 - val_loss: 144.9641 - val_mae: 144.9641 - val_mse: 8493648.0000\n",
      "Epoch 66/100\n",
      "373/373 - 1s - loss: 138.9372 - mae: 138.9372 - mse: 7685982.0000 - val_loss: 144.9574 - val_mae: 144.9574 - val_mse: 8501114.0000\n",
      "Epoch 67/100\n",
      "373/373 - 1s - loss: 138.9607 - mae: 138.9607 - mse: 7679952.0000 - val_loss: 144.9291 - val_mae: 144.9291 - val_mse: 8517341.0000\n",
      "Epoch 68/100\n",
      "373/373 - 1s - loss: 138.9899 - mae: 138.9899 - mse: 7692332.0000 - val_loss: 144.9423 - val_mae: 144.9423 - val_mse: 8538952.0000\n",
      "Epoch 69/100\n",
      "373/373 - 1s - loss: 138.8962 - mae: 138.8962 - mse: 7681950.5000 - val_loss: 145.0868 - val_mae: 145.0868 - val_mse: 8490860.0000\n",
      "Epoch 70/100\n",
      "373/373 - 1s - loss: 138.9207 - mae: 138.9207 - mse: 7690293.5000 - val_loss: 145.0470 - val_mae: 145.0470 - val_mse: 8515440.0000\n",
      "Epoch 71/100\n",
      "373/373 - 1s - loss: 138.9258 - mae: 138.9258 - mse: 7695315.0000 - val_loss: 144.8851 - val_mae: 144.8851 - val_mse: 8484579.0000\n",
      "6242/6242 - 2s - loss: 144.5739 - mae: 144.5739 - mse: 10386118.0000\n",
      "Testing set Mean Abs Error: 144.57 PC_total\n"
     ]
    }
   ],
   "source": [
    "def build_simon():\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mae',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "simon = build_simon()\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "# The patience parameter is number of epochs to check for improvement\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = simon.fit(X_train, y_train, \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=2, \n",
    "                    callbacks=[early_stop], batch_size = 1000)\n",
    "\n",
    "loss, mae, mse = simon.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} PC_total\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Simon on whole train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_NN = train_data[['user_statuses_count', 'hashtag_count', 'user_mentions_count', 'user_followers_count', 'user_friends_count', 'user_verified', 'text_length', 'hour', 'week_day', 'day']]\n",
    "y_train_data_NN = train_data['retweet_count']\n",
    "eval_data_NN = eval_data[['user_statuses_count', 'hashtag_count', 'user_mentions_count', 'user_followers_count', 'user_friends_count', 'user_verified', 'text_length', 'hour', 'week_day', 'day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "533/533 - 1s - loss: 143.5820 - mae: 143.5820 - mse: 6972210.0000 - val_loss: 156.3913 - val_mae: 156.3913 - val_mse: 16270606.0000\n",
      "Epoch 2/100\n",
      "533/533 - 1s - loss: 142.4876 - mae: 142.4876 - mse: 6941578.5000 - val_loss: 155.3657 - val_mae: 155.3657 - val_mse: 16232848.0000\n",
      "Epoch 3/100\n",
      "533/533 - 1s - loss: 141.4897 - mae: 141.4897 - mse: 6907235.0000 - val_loss: 154.3292 - val_mae: 154.3292 - val_mse: 16206430.0000\n",
      "Epoch 4/100\n",
      "533/533 - 1s - loss: 140.5910 - mae: 140.5910 - mse: 6875303.0000 - val_loss: 153.4673 - val_mae: 153.4673 - val_mse: 16125968.0000\n",
      "Epoch 5/100\n",
      "533/533 - 1s - loss: 140.0446 - mae: 140.0446 - mse: 6841524.0000 - val_loss: 153.0930 - val_mae: 153.0930 - val_mse: 16113397.0000\n",
      "Epoch 6/100\n",
      "533/533 - 1s - loss: 139.8104 - mae: 139.8104 - mse: 6824103.0000 - val_loss: 153.0998 - val_mae: 153.0998 - val_mse: 16105606.0000\n",
      "Epoch 7/100\n",
      "533/533 - 1s - loss: 139.6481 - mae: 139.6481 - mse: 6816892.0000 - val_loss: 152.9361 - val_mae: 152.9361 - val_mse: 16051250.0000\n",
      "Epoch 8/100\n",
      "533/533 - 1s - loss: 139.5951 - mae: 139.5951 - mse: 6811925.0000 - val_loss: 152.7963 - val_mae: 152.7963 - val_mse: 16055356.0000\n",
      "Epoch 9/100\n",
      "533/533 - 1s - loss: 139.4629 - mae: 139.4629 - mse: 6805615.5000 - val_loss: 152.7360 - val_mae: 152.7360 - val_mse: 16061382.0000\n",
      "Epoch 10/100\n",
      "533/533 - 1s - loss: 139.4208 - mae: 139.4208 - mse: 6804886.5000 - val_loss: 152.7578 - val_mae: 152.7578 - val_mse: 16057693.0000\n",
      "Epoch 11/100\n",
      "533/533 - 1s - loss: 139.3758 - mae: 139.3758 - mse: 6796029.5000 - val_loss: 152.6666 - val_mae: 152.6666 - val_mse: 16075220.0000\n",
      "Epoch 12/100\n",
      "533/533 - 1s - loss: 139.3023 - mae: 139.3023 - mse: 6801371.0000 - val_loss: 152.6606 - val_mae: 152.6606 - val_mse: 16036052.0000\n",
      "Epoch 13/100\n",
      "533/533 - 1s - loss: 139.2485 - mae: 139.2485 - mse: 6791375.5000 - val_loss: 152.7376 - val_mae: 152.7376 - val_mse: 16108570.0000\n",
      "Epoch 14/100\n",
      "533/533 - 1s - loss: 139.2630 - mae: 139.2630 - mse: 6802103.0000 - val_loss: 152.6067 - val_mae: 152.6067 - val_mse: 16047601.0000\n",
      "Epoch 15/100\n",
      "533/533 - 1s - loss: 139.1577 - mae: 139.1577 - mse: 6786853.0000 - val_loss: 152.5450 - val_mae: 152.5450 - val_mse: 16076511.0000\n",
      "Epoch 16/100\n",
      "533/533 - 1s - loss: 139.1571 - mae: 139.1571 - mse: 6794503.0000 - val_loss: 153.4442 - val_mae: 153.4442 - val_mse: 16059654.0000\n",
      "Epoch 17/100\n",
      "533/533 - 1s - loss: 139.1258 - mae: 139.1258 - mse: 6798571.0000 - val_loss: 152.5524 - val_mae: 152.5524 - val_mse: 16074403.0000\n",
      "Epoch 18/100\n",
      "533/533 - 1s - loss: 139.0172 - mae: 139.0172 - mse: 6790454.5000 - val_loss: 152.6293 - val_mae: 152.6293 - val_mse: 16029707.0000\n",
      "Epoch 19/100\n",
      "533/533 - 1s - loss: 139.0791 - mae: 139.0791 - mse: 6788688.0000 - val_loss: 152.5904 - val_mae: 152.5904 - val_mse: 16038112.0000\n",
      "Epoch 20/100\n",
      "533/533 - 1s - loss: 139.0129 - mae: 139.0129 - mse: 6785867.5000 - val_loss: 152.8262 - val_mae: 152.8262 - val_mse: 16009019.0000\n",
      "Epoch 21/100\n",
      "533/533 - 1s - loss: 139.0598 - mae: 139.0598 - mse: 6775606.0000 - val_loss: 152.6106 - val_mae: 152.6106 - val_mse: 16097915.0000\n",
      "Epoch 22/100\n",
      "533/533 - 1s - loss: 139.0139 - mae: 139.0139 - mse: 6784808.0000 - val_loss: 152.4228 - val_mae: 152.4228 - val_mse: 16079170.0000\n",
      "Epoch 23/100\n",
      "533/533 - 1s - loss: 139.0201 - mae: 139.0201 - mse: 6792698.5000 - val_loss: 152.4101 - val_mae: 152.4101 - val_mse: 16054613.0000\n",
      "Epoch 24/100\n",
      "533/533 - 1s - loss: 138.9645 - mae: 138.9645 - mse: 6786054.5000 - val_loss: 152.5196 - val_mae: 152.5196 - val_mse: 16042271.0000\n",
      "Epoch 25/100\n",
      "533/533 - 1s - loss: 138.9509 - mae: 138.9509 - mse: 6783933.0000 - val_loss: 152.4286 - val_mae: 152.4286 - val_mse: 16097894.0000\n",
      "Epoch 26/100\n",
      "533/533 - 1s - loss: 138.9395 - mae: 138.9395 - mse: 6795707.0000 - val_loss: 152.4286 - val_mae: 152.4286 - val_mse: 16046417.0000\n",
      "Epoch 27/100\n",
      "533/533 - 1s - loss: 138.9295 - mae: 138.9295 - mse: 6784448.5000 - val_loss: 152.2842 - val_mae: 152.2842 - val_mse: 16061701.0000\n",
      "Epoch 28/100\n",
      "533/533 - 1s - loss: 138.8974 - mae: 138.8974 - mse: 6784417.0000 - val_loss: 152.3766 - val_mae: 152.3766 - val_mse: 16079636.0000\n",
      "Epoch 29/100\n",
      "533/533 - 1s - loss: 138.8484 - mae: 138.8484 - mse: 6781794.0000 - val_loss: 152.3226 - val_mae: 152.3226 - val_mse: 16085513.0000\n",
      "Epoch 30/100\n",
      "533/533 - 1s - loss: 138.8097 - mae: 138.8097 - mse: 6782653.0000 - val_loss: 152.4196 - val_mae: 152.4196 - val_mse: 16101085.0000\n",
      "Epoch 31/100\n",
      "533/533 - 1s - loss: 138.8294 - mae: 138.8294 - mse: 6785796.0000 - val_loss: 152.3126 - val_mae: 152.3126 - val_mse: 16104269.0000\n",
      "Epoch 32/100\n",
      "533/533 - 1s - loss: 138.8031 - mae: 138.8031 - mse: 6787159.0000 - val_loss: 152.3518 - val_mae: 152.3518 - val_mse: 16068316.0000\n",
      "Epoch 33/100\n",
      "533/533 - 1s - loss: 138.7932 - mae: 138.7932 - mse: 6776765.0000 - val_loss: 152.2925 - val_mae: 152.2925 - val_mse: 16079218.0000\n",
      "Epoch 34/100\n",
      "533/533 - 1s - loss: 138.7746 - mae: 138.7746 - mse: 6776359.5000 - val_loss: 152.4846 - val_mae: 152.4846 - val_mse: 16028546.0000\n",
      "Epoch 35/100\n",
      "533/533 - 1s - loss: 138.7721 - mae: 138.7721 - mse: 6777737.5000 - val_loss: 152.3352 - val_mae: 152.3352 - val_mse: 16050165.0000\n",
      "Epoch 36/100\n",
      "533/533 - 1s - loss: 138.7643 - mae: 138.7643 - mse: 6778269.5000 - val_loss: 152.2014 - val_mae: 152.2014 - val_mse: 16087676.0000\n",
      "Epoch 37/100\n",
      "533/533 - 1s - loss: 138.7533 - mae: 138.7533 - mse: 6785409.0000 - val_loss: 152.3471 - val_mae: 152.3471 - val_mse: 16039740.0000\n",
      "Epoch 38/100\n",
      "533/533 - 1s - loss: 138.7357 - mae: 138.7357 - mse: 6781265.0000 - val_loss: 154.4919 - val_mae: 154.4919 - val_mse: 16044014.0000\n",
      "Epoch 39/100\n",
      "533/533 - 1s - loss: 138.7132 - mae: 138.7132 - mse: 6777328.5000 - val_loss: 152.2204 - val_mae: 152.2204 - val_mse: 16065402.0000\n",
      "Epoch 40/100\n",
      "533/533 - 1s - loss: 138.6716 - mae: 138.6716 - mse: 6781368.0000 - val_loss: 152.2418 - val_mae: 152.2418 - val_mse: 16088047.0000\n",
      "Epoch 41/100\n",
      "533/533 - 1s - loss: 138.6642 - mae: 138.6642 - mse: 6781177.0000 - val_loss: 152.5170 - val_mae: 152.5170 - val_mse: 16076037.0000\n",
      "Epoch 42/100\n",
      "533/533 - 1s - loss: 138.5707 - mae: 138.5707 - mse: 6778242.0000 - val_loss: 152.1755 - val_mae: 152.1755 - val_mse: 16096099.0000\n",
      "Epoch 43/100\n",
      "533/533 - 1s - loss: 138.6057 - mae: 138.6057 - mse: 6772740.5000 - val_loss: 152.0797 - val_mae: 152.0797 - val_mse: 16069948.0000\n",
      "Epoch 44/100\n",
      "533/533 - 1s - loss: 138.5982 - mae: 138.5982 - mse: 6776051.0000 - val_loss: 152.0269 - val_mae: 152.0269 - val_mse: 16067938.0000\n",
      "Epoch 45/100\n",
      "533/533 - 1s - loss: 138.5616 - mae: 138.5616 - mse: 6774012.5000 - val_loss: 152.1672 - val_mae: 152.1672 - val_mse: 16085685.0000\n",
      "Epoch 46/100\n",
      "533/533 - 1s - loss: 138.5535 - mae: 138.5535 - mse: 6774023.5000 - val_loss: 152.2645 - val_mae: 152.2645 - val_mse: 16044301.0000\n",
      "Epoch 47/100\n",
      "533/533 - 1s - loss: 138.5388 - mae: 138.5388 - mse: 6774684.0000 - val_loss: 152.1993 - val_mae: 152.1993 - val_mse: 16057970.0000\n",
      "Epoch 48/100\n",
      "533/533 - 1s - loss: 138.5080 - mae: 138.5080 - mse: 6776222.0000 - val_loss: 152.2004 - val_mae: 152.2004 - val_mse: 16061735.0000\n",
      "Epoch 49/100\n",
      "533/533 - 1s - loss: 138.5060 - mae: 138.5060 - mse: 6776120.0000 - val_loss: 152.4108 - val_mae: 152.4108 - val_mse: 16017814.0000\n",
      "Epoch 50/100\n",
      "533/533 - 1s - loss: 138.5093 - mae: 138.5093 - mse: 6772581.5000 - val_loss: 152.0039 - val_mae: 152.0039 - val_mse: 16064569.0000\n",
      "Epoch 51/100\n",
      "533/533 - 1s - loss: 138.4745 - mae: 138.4745 - mse: 6770107.0000 - val_loss: 152.1203 - val_mae: 152.1203 - val_mse: 16107146.0000\n",
      "Epoch 52/100\n",
      "533/533 - 1s - loss: 138.4535 - mae: 138.4535 - mse: 6773262.5000 - val_loss: 151.9215 - val_mae: 151.9215 - val_mse: 16093553.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "533/533 - 1s - loss: 138.4366 - mae: 138.4366 - mse: 6775774.0000 - val_loss: 151.9141 - val_mae: 151.9141 - val_mse: 16052038.0000\n",
      "Epoch 54/100\n",
      "533/533 - 1s - loss: 138.4596 - mae: 138.4596 - mse: 6774290.5000 - val_loss: 151.9391 - val_mae: 151.9391 - val_mse: 16055634.0000\n",
      "Epoch 55/100\n",
      "533/533 - 1s - loss: 138.4153 - mae: 138.4153 - mse: 6774765.0000 - val_loss: 151.9732 - val_mae: 151.9732 - val_mse: 16072205.0000\n",
      "Epoch 56/100\n",
      "533/533 - 1s - loss: 138.3929 - mae: 138.3929 - mse: 6769508.0000 - val_loss: 151.9838 - val_mae: 151.9838 - val_mse: 16065477.0000\n",
      "Epoch 57/100\n",
      "533/533 - 1s - loss: 138.4044 - mae: 138.4044 - mse: 6770286.0000 - val_loss: 152.0426 - val_mae: 152.0426 - val_mse: 16039242.0000\n",
      "Epoch 58/100\n",
      "533/533 - 1s - loss: 138.3874 - mae: 138.3874 - mse: 6771039.5000 - val_loss: 151.8826 - val_mae: 151.8826 - val_mse: 16059638.0000\n",
      "Epoch 59/100\n",
      "533/533 - 1s - loss: 138.3820 - mae: 138.3820 - mse: 6773200.0000 - val_loss: 152.0148 - val_mae: 152.0148 - val_mse: 16047297.0000\n",
      "Epoch 60/100\n",
      "533/533 - 1s - loss: 138.3459 - mae: 138.3459 - mse: 6771590.5000 - val_loss: 151.9544 - val_mae: 151.9544 - val_mse: 16076569.0000\n",
      "Epoch 61/100\n",
      "533/533 - 1s - loss: 138.3192 - mae: 138.3192 - mse: 6771734.0000 - val_loss: 152.9241 - val_mae: 152.9241 - val_mse: 15982264.0000\n",
      "Epoch 62/100\n",
      "533/533 - 1s - loss: 138.3810 - mae: 138.3810 - mse: 6763656.5000 - val_loss: 151.9638 - val_mae: 151.9638 - val_mse: 16056948.0000\n",
      "Epoch 63/100\n",
      "533/533 - 1s - loss: 138.2777 - mae: 138.2777 - mse: 6769011.0000 - val_loss: 152.3582 - val_mae: 152.3582 - val_mse: 16016864.0000\n",
      "Epoch 64/100\n",
      "533/533 - 1s - loss: 138.2582 - mae: 138.2582 - mse: 6763138.0000 - val_loss: 151.9302 - val_mae: 151.9302 - val_mse: 16040870.0000\n",
      "Epoch 65/100\n",
      "533/533 - 1s - loss: 138.2605 - mae: 138.2605 - mse: 6766457.0000 - val_loss: 151.8698 - val_mae: 151.8698 - val_mse: 16071899.0000\n",
      "Epoch 66/100\n",
      "533/533 - 1s - loss: 138.2422 - mae: 138.2422 - mse: 6759902.5000 - val_loss: 151.9864 - val_mae: 151.9864 - val_mse: 16036778.0000\n",
      "Epoch 67/100\n",
      "533/533 - 1s - loss: 138.2307 - mae: 138.2307 - mse: 6767974.0000 - val_loss: 151.8217 - val_mae: 151.8217 - val_mse: 16055110.0000\n",
      "Epoch 68/100\n",
      "533/533 - 1s - loss: 138.2453 - mae: 138.2453 - mse: 6772910.5000 - val_loss: 151.8439 - val_mae: 151.8439 - val_mse: 16050569.0000\n",
      "Epoch 69/100\n",
      "533/533 - 1s - loss: 138.2118 - mae: 138.2118 - mse: 6756727.5000 - val_loss: 151.7942 - val_mae: 151.7942 - val_mse: 16073493.0000\n",
      "Epoch 70/100\n",
      "533/533 - 1s - loss: 138.1616 - mae: 138.1616 - mse: 6762746.5000 - val_loss: 151.7581 - val_mae: 151.7581 - val_mse: 16083491.0000\n",
      "Epoch 71/100\n",
      "533/533 - 1s - loss: 138.1210 - mae: 138.1210 - mse: 6759135.0000 - val_loss: 151.7058 - val_mae: 151.7058 - val_mse: 16087715.0000\n",
      "Epoch 72/100\n",
      "533/533 - 1s - loss: 138.1093 - mae: 138.1093 - mse: 6771547.5000 - val_loss: 151.8035 - val_mae: 151.8035 - val_mse: 16100286.0000\n",
      "Epoch 73/100\n",
      "533/533 - 1s - loss: 138.1307 - mae: 138.1307 - mse: 6763928.5000 - val_loss: 151.7801 - val_mae: 151.7801 - val_mse: 16090596.0000\n",
      "Epoch 74/100\n",
      "533/533 - 1s - loss: 138.0654 - mae: 138.0654 - mse: 6758825.0000 - val_loss: 151.9827 - val_mae: 151.9827 - val_mse: 16100916.0000\n",
      "Epoch 75/100\n",
      "533/533 - 1s - loss: 138.1101 - mae: 138.1101 - mse: 6757699.0000 - val_loss: 151.6291 - val_mae: 151.6291 - val_mse: 16084483.0000\n",
      "Epoch 76/100\n",
      "533/533 - 1s - loss: 138.0520 - mae: 138.0520 - mse: 6768189.5000 - val_loss: 151.8411 - val_mae: 151.8411 - val_mse: 16049851.0000\n",
      "Epoch 77/100\n",
      "533/533 - 1s - loss: 138.0581 - mae: 138.0581 - mse: 6761263.0000 - val_loss: 151.7470 - val_mae: 151.7470 - val_mse: 16062711.0000\n",
      "Epoch 78/100\n",
      "533/533 - 1s - loss: 137.9991 - mae: 137.9991 - mse: 6764516.5000 - val_loss: 151.9365 - val_mae: 151.9365 - val_mse: 16056360.0000\n",
      "Epoch 79/100\n",
      "533/533 - 1s - loss: 137.9143 - mae: 137.9143 - mse: 6755557.0000 - val_loss: 151.9034 - val_mae: 151.9034 - val_mse: 16036569.0000\n",
      "Epoch 80/100\n",
      "533/533 - 1s - loss: 137.9414 - mae: 137.9414 - mse: 6751795.0000 - val_loss: 152.5667 - val_mae: 152.5667 - val_mse: 15999073.0000\n",
      "Epoch 81/100\n",
      "533/533 - 1s - loss: 138.0332 - mae: 138.0332 - mse: 6750984.0000 - val_loss: 151.8789 - val_mae: 151.8789 - val_mse: 16047821.0000\n",
      "Epoch 82/100\n",
      "533/533 - 1s - loss: 137.9639 - mae: 137.9639 - mse: 6758668.0000 - val_loss: 151.7313 - val_mae: 151.7313 - val_mse: 16085536.0000\n",
      "Epoch 83/100\n",
      "533/533 - 1s - loss: 137.8983 - mae: 137.8983 - mse: 6757860.5000 - val_loss: 151.7375 - val_mae: 151.7375 - val_mse: 16064746.0000\n",
      "Epoch 84/100\n",
      "533/533 - 1s - loss: 137.9432 - mae: 137.9432 - mse: 6746314.0000 - val_loss: 151.8482 - val_mae: 151.8482 - val_mse: 16110799.0000\n",
      "Epoch 85/100\n",
      "533/533 - 1s - loss: 137.9425 - mae: 137.9425 - mse: 6756882.5000 - val_loss: 151.8976 - val_mae: 151.8976 - val_mse: 16074142.0000\n"
     ]
    }
   ],
   "source": [
    "def build_simon():\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mae',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "simon = build_simon()\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "# The patience parameter is number of epochs to check for improvement\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = simon.fit(train_data_NN, y_train_data_NN, \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=2, \n",
    "                    callbacks=[early_stop], batch_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting #retweets for evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = simon.predict(eval_data_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"FILE_NAME.txt\", 'w') as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "#    for index, prediction in enumerate(y_pred):\n",
    "#        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
