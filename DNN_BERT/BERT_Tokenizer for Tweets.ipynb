{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Objective is to **tokenize** the texts using BERT tokenizer in order to feed them to our Neural Network **DNN_BERT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from local scripts:\n",
    "import sys\n",
    "# insert at 1 the script path:\n",
    "sys.path.insert(1, 'Scripts')\n",
    "from Training_Accuracy_functions import accuracy, train_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"../Data/train.csv\")\n",
    "\n",
    "# Load the test data:\n",
    "eval_data = pd.read_csv(\"../Data/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  I love Ronan Sangouard\n",
      "Tokenized:  ['i', 'love', 'ronan', 'sang', '##ou', '##ard']\n",
      "Token IDs:  [1045, 2293, 18633, 6369, 7140, 4232]\n"
     ]
    }
   ],
   "source": [
    "text = \"I love Ronan Sangouard\"\n",
    "\n",
    "# Print the original sentence.\n",
    "print(' Original: ', text)\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(text))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMaxlen(data, tokenizer):\n",
    "    max_len = 0\n",
    "\n",
    "    # For every sentence\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "\n",
    "        # Update the maximum sentence length.\n",
    "        max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "    return max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "665777it [04:20, 2551.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', ComputeMaxlen(train_data, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def TokenizeData(data, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            row['text'],                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            padding = 'max_length',\n",
    "                            max_length = 512,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    return input_ids, attention_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "665777it [05:33, 1995.32it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = TokenizeData(train_data, tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our new reality. Social distancing. #COVID19 #SocialDistance https://t.co/upR0Z7X0a2\n",
      "Token IDs: tensor([  101.,  2256.,  2047.,  4507.,  1012.,  2591.,  4487., 12693.,  6129.,\n",
      "         1012.,  1001.,  2522., 17258., 16147.,  1001.,  2591., 10521., 26897.,\n",
      "        16770.,  1024.,  1013.,  1013.,  1056.,  1012.,  2522.,  1013.,  2039.,\n",
      "         2099.,  2692.,  2480.,  2581.,  2595.,  2692.,  2050.,  2475.,   102.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids).float().cuda()\n",
    "#attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(train_data['retweet_count'])\n",
    "\n",
    "# Print sentence 32, now as a list of IDs.\n",
    "print('Original: ', train_data['text'][32])\n",
    "print('Token IDs:', input_ids[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285334it [01:52, 2543.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', ComputeMaxlen(eval_data, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285334it [02:24, 1981.14it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids_test, _ = TokenizeData(eval_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test).float().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Also in this poll, only 52% said they wear masks inside public spaces 😷 not great\n",
      "Token IDs: tensor([  101.,  2036.,  1999.,  2023.,  8554.,  1010.,  2069.,  4720.,  1003.,\n",
      "         2056.,  2027.,  4929., 15806.,  2503.,  2270.,  7258.,   100.,  2025.,\n",
      "         2307.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 32, now as a list of IDs.\n",
    "print('Original: ', eval_data['text'][32])\n",
    "print('Token IDs:', input_ids_test[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tr = pd.read_pickle(\"../Preprocessing/Data/train_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding them to tensor\n",
    "features_arr = np.array(features_tr[['user_statuses_count', 'hashtag_count', 'user_mentions_count', 'user_followers_count', 'user_friends_count', 'user_verified', 'text_length', 'hour', 'week_day', 'day']])\n",
    "input_ids[:,-11:-1] = torch.tensor(features_arr, device = 'cuda').float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = pd.read_pickle(\"../Preprocessing/Data/eval_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding them to tensor\n",
    "features_arr_test = np.array(features_test[['user_statuses_count', 'hashtag_count', 'user_mentions_count', 'user_followers_count', 'user_friends_count', 'user_verified', 'text_length', 'hour', 'week_day', 'day']])\n",
    "input_ids_test[:,-11:-1] = torch.tensor(features_arr_test, device = 'cuda').float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_ids, 'Tensors/Training/Tokens_10features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labels, 'Tensors/Training/Retweets.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_ids_test, 'Tensors/Testing/Tokens_10features.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links:\n",
    "\n",
    "1. For a conceptual understanding of bert click [here](https://towardsdatascience.com/bert-to-the-rescue-17671379687f).\n",
    "2. You can also find an example [here](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
    "3. Simple BERT Explanation [here](https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/#:~:text=What%20is%20BERT%3F,task%2Dspecific%20fine%2Dtuning.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
